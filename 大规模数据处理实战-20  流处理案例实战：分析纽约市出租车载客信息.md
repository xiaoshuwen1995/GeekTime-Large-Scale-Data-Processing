# 20 | 流处理案例实战：分析纽约市出租车载客信息

下面就用 Structured Streaming 对纽约市出租车的载客信息进行处理，建立一个实时流处理的 pipeline。

实时输出各个区域内乘客小费的平均数，来帮助司机决定要去哪里接单。

## 数据集介绍

用到的数据集为纽约市 2009～2015 年出租车载客的信息，分别是地理位置信息和付费信息。

数据集1：每一次出行包含了两个事件：出发、到达。出发和到达这两个事件都有 11 个属性， schema 如下所示：

![img](https://static001.geekbang.org/resource/image/4a/90/4ae9c7d353925f84d36bf7280f2b5b90.jpg)

数据集2：所有出租车的付费信息，它有 8 个属性，schema 如下所示：

![img](https://static001.geekbang.org/resource/image/8e/de/8ef443617788243f4546116fffb40ede.jpg)

这个数据集下载网址是https://training.ververica.com/setup/taxiData.html，数据集的规模在 100MB 左右。它只是节选了一部分出租车的载客信息，所以在本机运行就可以了。

详细的纽约出租车数据集超过了 500GB，下载网址是https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page。

## 流数据输入

Apache Kafka是基于 Pub/Sub 模式的流数据处理平台。在这个例子中，Consumer 是之后要写的 Spark 流处理程序，这个消息队列有两个 Topic，一个包含出行的地理位置信息，一个包含出行的收费信息。

Kafka 会按照时间顺序，向这两个 Topic 中发布事件，从而模拟一个实时的流数据源。

```Python

from pyspark.sql import SparkSession

spark = SparkSession.builder
   .appName("Spark Structured Streaming for taxi ride info")
   .getOrCreate()

rides = spark
   .readStream
   .format("kafka")
   .option("kafka.bootstrap.servers", "localhost:xxxx") //取决于Kafka的配置
   .option("subscribe", "taxirides")
   .option("startingOffsets", "latest")
   .load()
   .selectExpr("CAST(value AS STRING)")

fares = spark
   .readStream
   .format("kafka")
   .option("kafka.bootstrap.servers", "localhost:xxxx")
   .option("subscribe", "taxifares")
   .option("startingOffsets", "latest")
   .load()
   .selectExpr("CAST(value AS STRING)
```

下面是数据清洗。要想分离出我们需要的位置和付费信息，我们首先要把数据分割成一个个属性，并创建到对应的 DataFrame 中的列。

首先，要根据数据类型创建对应的 schema。

```Python

ridesSchema = StructType([
   StructField("rideId", LongType()), StructField("isStart", StringType()),
   StructField("endTime", TimestampType()), StructField("startTime", TimestampType()),
   StructField("startLon", FloatType()), StructField("startLat", FloatType()),
   StructField("endLon", FloatType()), StructField("endLat", FloatType()),
   StructField("passengerCnt", ShortType()), StructField("taxiId", LongType()),
   StructField("driverId", LongType())])

faresSchema = StructType([
   StructField("rideId", LongType()), StructField("taxiId", LongType()),
   StructField("driverId", LongType()), StructField("startTime", TimestampType()),
   StructField("paymentType", StringType()), StructField("tip", FloatType()),
   StructField("tolls", FloatType()), StructField("totalFare", FloatType())])

def parse_data_from_kafka_message(sdf, schema):
   from pyspark.sql.functions import split
   assert sdf.isStreaming == True, "DataFrame doesn't receive streaming data"
   col = split(sdf['value'], ',')
   for idx, field in enumerate(schema):
       sdf = sdf.withColumn(field.name, col.getItem(idx).cast(field.dataType))
   return sdf.select([field.name for field in schema])

rides = parse_data_from_kafka_message(rides, ridesSchema)
fares = parse_data_from_kafka_message(fares, faresSchema)
```

在上面的代码中，我们定义了函数 parse_data_from_kafka_message，用来把 Kafka 发来的 message 根据 schema 拆成对应的属性，转换类型，并加入到 DataFrame 的表中。

```Python
MIN_LON, MAX_LON, MIN_LAT, MAX_LAT = -73.7, -74.05, 41.0, 40.5
rides = rides.filter(
   rides["startLon"].between(MIN_LON, MAX_LON) &
   rides["startLat"].between(MIN_LAT, MAX_LAT) &
   rides["endLon"].between(MIN_LON, MAX_LON) &
   rides["endLat"].between(MIN_LAT, MAX_LAT))
rides = rides.filter(rides["isStart"] == "END")
```

在 Spark 2.3 中，流与流的 Join（Stream-stream join）被正式支持。这样的 Join 难点就在于，在任意一个时刻，流数据都不是完整的，流 A 中后面还没到的数据有可能要和流 B 中已经有的数据 Join 起来再输出。为了解决这个问题，我们就要引入数据水印（Watermark）的概念。

数据水印定义了我们可以对数据延迟的最大容忍限度。

比如说，如果定义水印是 10 分钟，数据 A 的事件时间是 1:00，数据 B 的事件时间是 1:10。由于数据传输发生了延迟，我们在 1:15 才收到了 A 和 B，那么我们将只处理数据 B 并更新结果，A 会被无视。

在 Join 操作中，好好利用水印，我们就知道什么时候可以不用再考虑旧数据，什么时候必须把旧数据保留在内存中。不然，我们就必须把所有旧数据一直存在内存里，导致数据不断增大，最终可能会内存泄漏。

```Python

faresWithWatermark = fares
   .selectExpr("rideId AS rideId_fares", "startTime", "totalFare", "tip")
   .withWatermark("startTime", "30 minutes")

ridesWithWatermark = rides
 .selectExpr("rideId", "endTime", "driverId", "taxiId", "startLon", "startLat", "endLon", "endLat")
 .withWatermark("endTime", "30 minutes")

joinDF = faresWithWatermark
   .join(ridesWithWatermark,
     expr("""
      rideId_fares = rideId AND
       endTime > startTime AND
       endTime <= startTime + interval 2 hours
       """)
```

在这段代码中，我们对 fares 和 rides 分别加了半小时的水印，然后把两个 DataFrame 根据 rideId 和时间间隔的限制 Join 起来。这样，joinDF 就同时包含了地理位置和付费信息。

滑动窗口操作，是流处理中常见的输出形式，即输出每隔一段时间内，特定时间窗口的特征值。

我们可以每隔 10 分钟，输出过去半小时内每个区域内的平均小费。这样的话，司机可以每隔 10 分钟查看一下数据，决定下一步去哪里接单。这个查询（Query）可以由以下代码产生。

```Python

tips = joinDF
   .groupBy(
       window("endTime", "30 minutes", "10 minutes"),
       "area")
   .agg(avg("tip"))
   
query.writeStream
   .outputMode("append")
   .format("console")
   .option("truncate", False
   .start()
   .awaitTermination()
```

为什么我们不把输出结果按小费多少进行排序呢？这是因为两个流的 inner-join 只支持附加输出模式（Append Mode），而现在 Structured Streaming 不支持在附加模式下进行排序操作。

希望将来 Structured Streaming 可以提供这个功能。但现在司机们只能扫一眼所有的输出数据在心里大概判断。

## 小结

流处理更加复杂，对延迟性要求更高。Spark 最大的好处之一就是它拥有统一的批流处理框架和 API。

## 思考题

为什么流的 Inner-Join 不支持完全输出模式?

对于 Inner-Join 而言，加水印是否是必须的？ Outer-Join 呢？

现阶段不仅Inner-join不支持完全输出模式，任何类型的Join都不支持完全输出模式。因为完全输出模式要求每当有新数据输入时，输出完整的结果表。而对于无边界数据，我们很难把所有历史数据存在内存中。所以，一般Join的都是在某个时间窗口内的流数据，这就是引入watermarking的原因。希望将来Spark可以引入新的机制来支持这一点。

Outer join是要在Inner Join的基础上，把没有匹配的行的对应列设为NULL。但是由于流数据的无边界性，Spark永远无法知道在未来会不会找到匹配的数据。所以为了保证Outer Join的正确性，加水印是必须的。这样Spark的执行引擎只要在水印的有效期内没找到与之匹配的数据，就可以把对应的列设为NULL并输出。

由于Inner Join不需要连接两个表中所有的行，所以在Spark官网的叙述中，水印和事件时间的限制不是必须的。但是如果不加任何限制，流数据会不断被读入内存，这样不安全的。所以，即便是Inner Join，我也推荐你加水印和事件时间的限制。