大规模数据处理技术是在放眼未来的几十年中都依然会是炙手可热的一项技术，不会被淘汰。

## 技术迭代带来的烦恼

而在 2009 年，Spark 这个计算框架在加州伯克利大学的 AMPLab 实验室中诞生。2010 年，Spark 正式开源了。“Spark 的数据处理效率远在 Hadoop 之上”的结论经过了业界的验证。2014 年，Spark 更是成为了 Apache 的顶级项目。

一种做法是放弃现有的技术框架，重新花费大量时间去学习新的数据处理框架。这太累了。对于工程师来说，平时本来就有着做不完的任务和业绩压力，还需要抽空学习新的技术和进行代码的迁移，这无疑让工程师们有着非常大的压力和负担。

当然，还有一种做法是保持现有的技术框架，不断优化现有基础设施，并且寄希望于老框架可以有新的功能发布让其性能得以提高。

讲到这里，你不难发现，当有新的技术框架出现的时候，工程师就会陷入一个选择的困难，纠结到底是抛弃原有的技术架构，还是花大量时间去做技术迁移。

其实，如果一开始就有 Beam 模型存在的话，你可能就不必有这个烦恼了。因为我们完全不需要担心某一个 Runner，也就是具体的数据处理技术框架过时之后所带来的技术迁移成本。如果你想要完成底层处理框架的迁移，只需要更改一些 Runner 的接口就可以了。