# 大规模数据处理实战-03 | 大规模数据处理初体验：怎样实现大型电商热销榜？

我在 Google 面试过很多优秀的候选人，普通编程问题的coding应用能力很强，算法和数据结构也应用得不错。可是当我追问数据规模变大时该怎么设计系统，他们却说不出所以然来。

缺乏必备的**规模增长的技术思维（mindset of scaling）**，会限制这些候选人的职业成长。

## 小规模的经典算法

怎样设计一个系统，根据销售记录统计去年销量前 10 的商品？这个问题的解法分为两步：

1. 统计每个商品的销量。可以用哈希表（hashtable）数据结构来解决，是一个 O(n) 的算法，例如 n 是 1000 亿。

2. 找出销量前十。可以用经典的 Top K 算法，也是 O(n) 的算法。


在小规模系统中，我们完全可以用经典简洁的算法来解决。但随着系统尺度的变大，很多方法就不再适用。

*【比如，在小尺度经典物理学中适用的牛顿力学公式是这样的：*
$$
F=ma
$$
*但公司这在高速强力的物理系统中就不再适用，在狭义相对论中有另外的表达。*

*在社会系统中也是一样，管理 10 人团队和治理 14 亿人口的国家，复杂度也不可同日而语。】*

具体在我们这个问题中，当数据规模从GB增长到TB时，应用算法会遇到哪些问题呢？

1. 内存占用问题。对于 TB 级的交易记录数据，很难用单台计算机容纳这么大的哈希表。可能想到的解决方法是放弃哈希表，而是在磁盘里完成销量计数。比如，就用一个 1000 亿行的文件或者表，然后再把销量统计结果一行一行读进后面的堆树 / 优先级队列。理论上听起来不错，实际上是否真的可行呢，那我们看下一点。

2. 磁盘 I/O 等延时问题。当数据规模变大时，我们需要把一些中间结果存进磁盘，以应对单步任务出错等问题。一次磁盘读取大概需要 10ms 的时间，利用上一点提到的替代方法，因为我们是一个 O(n * log k) 的算法，就需要 $10ms * 10^9 = 10 ^ 7 s = 115 Day$的时间，是完全不现实的。


当单台机器已经无法适应大规模的数据，如何解决内存占用和耗时问题呢？我们的解决方案是做**横向扩展（Scale-out）**。

## 大规模分布式解决方案

依然延续之前的两步思路，我们只需要把每一步简单的函数算法，升级为计算集群的分布式算法。

![img](https://static001.geekbang.org/resource/image/3e/af/3eaea261df4257f0cff4509d82f211af.png)

### 统计每个商品的销量

我们需要的第一个计算集群，是统计商品销量的集群。例如，每次能处理 1 万条销售记录的1000台机器。对于每台机器而言，它的单次处理数据规模大大缩小，因此可以回归到使用熟悉的传统算法。

例如下图每台机器的输入是 2 条销售记录，输出是产品销量计数。

![img](https://static001.geekbang.org/resource/image/8e/8a/8eeff3376743e886d5f2d481ca8ddb8a.jpg)

### 找出销量前 K

我们需要的第二个计算集群，则是找出销量前十的集群。这里我们先把问题抽象为找出销量前 K 的产品。在上一个统计销量集群里得到的销量统计数据输出，将会是我们这个处理流程的输入。我们需要做的是分布在各个机器的产品销量汇总。

下图示例是 K = 1 的情况，每台机器按照product_id 分组。先把所有 product_id 的销量相加，再输出自己机器上销量最高（K=1）的商品。

![img](https://static001.geekbang.org/resource/image/38/fc/38933e25ca315bd56321753573d5bbfc.jpg)

### 汇总最终结果

到了最后一步，是汇总“销量前 K 集群”中的结果。这一步从所有候选者中找出真正的销量前 K=1 的商品，因此完全可以用单一机器解决。

例子中实际做的就是这 1000 台机器的结果汇总，规模足够小。

![img](https://static001.geekbang.org/resource/image/ca/ab/cab28c9e3ba9031072a4e6949328bbab.jpg)

## 大规模数据处理框架的功能要求

大规模数据处理框架，两个最基本的需求是：

1. 高度抽象的数据处理流程描述语言。作为运维管理者，肯定不想再逐一配置分布式系统的每台机器；作为框架使用者，也希望能够用简单的几行代码把业务逻辑描述清楚。
2. 分配数据处理流程任务的自动优化。希望支撑这个框架的引擎能够足够智能，减少需要手动配置的系统，实现自动任务分配。

## 小结

这一讲中，我们粗浅地分析了一个电商排行榜的数据处理例子。从 GB 数据到 TB 数据，我们从小规模算法升级到了分布式处理的设计方案；从单一 TB 数据场景到 1000 个应用场景，我们探索了大规模数据处理框架的设计。这些都是为了帮助你更好地理解后面所要讲的所有知识。比如，为什么传统算法不再奏效？为什么要去借助抽象的数据处理描述语言？希望在后面的学习过程中，你能一直带着这些问题出发。

> 最初，GPS数据以文件形式存储在盘阵中，数据增长达到TB级别后，考虑到性能和成本以及可扩展性，系统迁移到HDFS中，离线任务用MR，在线查询采用HBSE，现在，数据PB级别后，发现热点数据hbase成本太高，系统迁移到时序数据库，专供线上实时查询，同时，实时分析采用storm，批处理用spark。其实，很多情况下，采用什么技术，成本具有决定性因素